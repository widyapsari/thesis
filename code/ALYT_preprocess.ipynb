{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fbc8337-726f-4edd-a781-87312616282b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Widya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd5310fc-43a4-4bfd-ba91-4df4ad5f614a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv( \"../data/ALYT_master.csv\")\n",
    "\n",
    "## drop unnecessary column and drop label 4 (uncertain) ##\n",
    "df = df.drop([\"comment_id\", \"label_normalised\", \"video_id\", \"video_topic\", \"video_type\"], axis=1)\n",
    "df = df[df['label'] != 4]\n",
    "\n",
    "## map class number to abuse/no abuse ##\n",
    "conditions = [\n",
    "    (df['label'] >= 1) & (df['label'] <= 3),\n",
    "    (df['label'] >= 5) & (df['label'] <= 7)\n",
    "]\n",
    "\n",
    "choices = ['NOT', 'ABU']\n",
    "\n",
    "# Create a new column 'label2' based on the conditions, keeping the original 'label' where conditions are not met\n",
    "df['label2'] = np.select(conditions, choices, default=df['label'])\n",
    "df.to_csv(\"../data/alyt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb24aa58-b8b3-4edf-8cb4-fd8acc2770fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_comment(comment): \n",
    "    \"\"\"\n",
    "    This function is used to clean the sentence froom column \"comment\" dataframe alyt\n",
    "    :type comment: string\n",
    "    \"\"\"\n",
    "    comment = re.sub(r\"@[A-Za-z0-9]+\", '', comment)\n",
    "    comment = re.sub(\"#\", \"\", tweet)\n",
    "    # tweet = re.sub(r\"[^a-zA-Z\\s]\", \"\", tweet) #Removes also inner *, but also emoticones\n",
    "    comment = re.sub(r\"\\s[^a-zA-Z]\\s\", \" \", comment) #This and 2 lower lines do not remove inner *, but also not emoticones\n",
    "    comment = re.sub(r\"\\s[^a-zA-Z]\", \" \", comment)\n",
    "    comment = re.sub(r\"[^a-zA-Z]\\s\", \" \", comment)\n",
    "    comment = comment.strip('*')\n",
    "    comment = comment.lower()\n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     stop_words = set(stopwords.words('english'))  \n",
    "#     words = tweet.split()\n",
    "#     words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "#     tweet = ' '.join(words)\n",
    "    return comment\n",
    "\n",
    "df_alyt = pd.read_csv(\"../data/alyt.csv\")\n",
    "df_alyt['comment'] = df_alyt['comment'].apply(clean_tweet)\n",
    "df_alyt.to_csv(\"../data/alyt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e88df62d-3676-4a33-81de-82e99a9b1388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## splitting the dataset into train and test. Ratio 70:30 ##\n",
    "df_alyt = pd.read_csv(\"../data/alyt.csv\")\n",
    "\n",
    "X = df_alyt [\"comment\"]\n",
    "y = df_alyt[\"label2\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) \n",
    "\n",
    "## save train data ##\n",
    "data_train = {\n",
    "    \"comment\" : X_train,\n",
    "    \"label\" : y_train}\n",
    "\n",
    "df = pd.DataFrame(data_train)\n",
    "df.to_csv(\"../data/ALYT_train.csv\", index = False)\n",
    "\n",
    "## save test data ##\n",
    "data_test = {\n",
    "    \"comment\" : X_test,\n",
    "    \"label\" : y_test}\n",
    "df = pd.DataFrame(data_test)\n",
    "df.to_csv(\"../data/ALYT_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0322eafd-8979-48e8-a477-64b8ce089a39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "NOT    11958\n",
      "ABU     1619\n",
      "Name: count, dtype: int64\n",
      "\n",
      "label\n",
      "NOT    5164\n",
      "ABU     655\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/ALYT_train.csv\")\n",
    "test = pd.read_csv(\"../data/ALYT_test.csv\")\n",
    "print(train.label.value_counts())\n",
    "print()\n",
    "print(test.label.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
